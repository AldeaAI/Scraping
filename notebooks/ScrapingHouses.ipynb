{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d999f46d-6b53-4fdf-964f-f3dceb118a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping listings from: https://www.lalonjapropiedadraiz.com/inmuebles/Venta/clases_Casa\n",
      "Scraping listings from: https://www.lalonjapropiedadraiz.com/inmuebles/Venta/clases_Casa/pagina/2\n",
      "Scraping listings from: https://www.lalonjapropiedadraiz.com/inmuebles/Venta/clases_Casa/pagina/3\n",
      "Scraping data from: https://www.lalonjapropiedadraiz.com/inmueble/9960/Venta/\n",
      "Scraping data from: https://www.lalonjapropiedadraiz.com/inmueble/10159/Venta/\n",
      "Scraping data from: https://www.lalonjapropiedadraiz.com/inmueble/7398/Venta/\n",
      "Scraping data from: https://www.lalonjapropiedadraiz.com/inmueble/9061/Venta/\n",
      "Scraping data from: https://www.lalonjapropiedadraiz.com/inmueble/9634/Venta/\n",
      "Scraping data from: https://www.lalonjapropiedadraiz.com/inmueble/10040/Venta/\n",
      "Scraping data from: https://www.lalonjapropiedadraiz.com/inmueble/9537/Venta/\n",
      "Scraping data from: https://www.lalonjapropiedadraiz.com/inmueble/10269/Venta/\n",
      "Scraping data from: https://www.lalonjapropiedadraiz.com/inmueble/9973/Venta/\n",
      "Scraping data from: https://www.lalonjapropiedadraiz.com/inmueble/9562/Venta/\n",
      "Scraping data from: https://www.lalonjapropiedadraiz.com/inmueble/9554/Venta/\n",
      "Scraping data from: https://www.lalonjapropiedadraiz.com/inmueble/10271/Venta/\n",
      "Scraping data from: https://www.lalonjapropiedadraiz.com/inmueble/9577/Venta/\n",
      "Scraping data from: https://www.lalonjapropiedadraiz.com/inmueble/8806/Venta/\n",
      "Scraping data from: https://www.lalonjapropiedadraiz.com/inmueble/9817/Venta/\n",
      "Scraping data from: https://www.lalonjapropiedadraiz.com/inmueble/8005/Venta/\n",
      "Scraping data from: https://www.lalonjapropiedadraiz.com/inmueble/9833/Venta/\n",
      "Scraping data from: https://www.lalonjapropiedadraiz.com/inmueble/9599/Venta/\n",
      "Scraping data from: https://www.lalonjapropiedadraiz.com/inmueble/9437/Venta/\n",
      "Scraping data from: https://www.lalonjapropiedadraiz.com/inmueble/9190/Venta/\n",
      "Scraping data from: https://www.lalonjapropiedadraiz.com/inmueble/10315/Venta/\n",
      "Scraping data from: https://www.lalonjapropiedadraiz.com/inmueble/9413/Venta/\n",
      "Scraping data from: https://www.lalonjapropiedadraiz.com/inmueble/9165/Venta/\n",
      "Scraping data from: https://www.lalonjapropiedadraiz.com/inmueble/9412/Venta/\n",
      "Scraping data from: https://www.lalonjapropiedadraiz.com/inmueble/9400/Venta/\n",
      "Scraping data from: https://www.lalonjapropiedadraiz.com/inmueble/9636/Venta/\n",
      "Scraping data from: https://www.lalonjapropiedadraiz.com/inmueble/10197/Venta/\n",
      "Scraping data from: https://www.lalonjapropiedadraiz.com/inmueble/9146/Venta/\n",
      "Scraping data from: https://www.lalonjapropiedadraiz.com/inmueble/9309/Venta/\n",
      "Scraping data from: https://www.lalonjapropiedadraiz.com/inmueble/10403/Venta/\n",
      "Scraping data from: https://www.lalonjapropiedadraiz.com/inmueble/9411/Venta/\n",
      "Scraping data from: https://www.lalonjapropiedadraiz.com/inmueble/10164/Venta/\n",
      "Data saved to listings_data_2025-01-02.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def get_total_pages(url):\n",
    "    \"\"\"\n",
    "    Fetches the given URL and extracts the total number of pages from the pagination.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the website to scrape.\n",
    "\n",
    "    Returns:\n",
    "        int: The total number of pages if found, None otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        last_page_link = soup.select_one(\".pagination li:last-child a\") #Selects the last page link\n",
    "\n",
    "        if last_page_link:\n",
    "            try:\n",
    "                last_page_text = last_page_link.text.strip()\n",
    "                if last_page_text == \"»\": #Handles the case where the last page is represented by »\n",
    "                    last_page_link = soup.select(\".pagination li:nth-last-child(2) a\") #Selects the second to last element\n",
    "                    if last_page_link:\n",
    "                        last_page_text = last_page_link[0].text.strip()\n",
    "                return int(last_page_text)\n",
    "            except ValueError:\n",
    "                return None  # Return None if the last page text is not a valid integer\n",
    "        else:\n",
    "            return None  # Return None if no pagination is found\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching URL: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_all_page_links(base_url, total_pages):\n",
    "    \"\"\"\n",
    "    Creates a list of URLs for all pages based on the base URL and total number of pages.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): The base URL of the website.\n",
    "        total_pages (int): The total number of pages.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of URLs for all pages.\n",
    "    \"\"\"\n",
    "    all_links = [base_url]  # Start with the base URL\n",
    "    for page_num in range(2, total_pages + 1):\n",
    "        page_url = f\"{base_url}/pagina/{page_num}\"\n",
    "        all_links.append(page_url)\n",
    "    return all_links\n",
    "\n",
    "def get_listing_links(page_url, base_url):\n",
    "    \"\"\"\n",
    "    Extracts listing links and their codes from a given page, excluding those containing \"Ambos\".\n",
    "\n",
    "    Args:\n",
    "        page_url (str): The URL of the page to scrape.\n",
    "        base_url (str): The base URL of the website.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each containing 'link' and 'code' for a listing.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(page_url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        listing_elements = soup.find_all(id=\"ruta32\") #Find all elements with the id ruta32\n",
    "        listings = []\n",
    "        for element in listing_elements:\n",
    "            relative_link = element.get('href') #Gets the href of the element\n",
    "            if relative_link:\n",
    "                full_link = urljoin(base_url, relative_link) #Joins the relative link with the base url\n",
    "                if \"Ambos\" not in full_link: #Checks that the link does not contain the word Ambos\n",
    "                    match = re.search(r\"/inmueble/(\\d+)/\", full_link) #Extracts the code using regex\n",
    "                    property_code = match.group(1) if match else None #Gets the code from the regex match\n",
    "                    listings.append({\"link\": full_link, \"code\": property_code}) #Appends a dictionary with the link and the code\n",
    "        return listings\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching URL: {e}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_listing_data(listing_info):\n",
    "    \"\"\"\n",
    "    Fetches data from a single listing page and adds the extraction date.\n",
    "\n",
    "    Args:\n",
    "        listing_info (dict): A dictionary containing the 'link' and 'code' of the listing.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the extracted data, or None if an error occurs or data is incomplete.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        listing_url = listing_info[\"link\"]\n",
    "        response = requests.get(listing_url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        data = listing_info.copy()\n",
    "        price_element = soup.find(class_=\"property-price\")\n",
    "        data[\"Precio\"] = price_element.text.strip() if price_element else None\n",
    "\n",
    "        data[\"Baños\"] = extract_value(soup, \"Baños\")\n",
    "        data[\"Área\"] = extract_value(soup, \"Área\")\n",
    "        data[\"Habitaciones\"] = extract_value(soup, \"Habitaciones\")\n",
    "        data[\"Garajes\"] = extract_value(soup, \"Garajes\")\n",
    "        data[\"Closets\"] = extract_value(soup, \"Closets\")\n",
    "\n",
    "        # Extract and Split Location\n",
    "        location_element = soup.find(class_=\"listing-address\")\n",
    "        location = location_element.text.strip() if location_element else None\n",
    "        if location:\n",
    "            parts = location.split(\",\", 1)  # Split at the first comma only\n",
    "            data[\"Municipio\"] = parts[0].strip()\n",
    "            data[\"Barrio\"] = parts[1].strip() if len(parts) > 1 else None #Handles cases where there is no barrio\n",
    "        else:\n",
    "            data[\"Municipio\"] = None\n",
    "            data[\"Barrio\"] = None\n",
    "\n",
    "        if all(data.values()) and \"-\" not in data[\"Precio\"]:\n",
    "            data[\"Extraction Date\"] = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "            return data\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching listing URL: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while parsing: {e}\")\n",
    "        return None\n",
    "        \n",
    "def extract_value(soup, label):\n",
    "    \"\"\"\n",
    "    Extracts a value based on a label from the page HTML using multiple strategies.\n",
    "\n",
    "    Args:\n",
    "        soup (BeautifulSoup): The BeautifulSoup object representing the page HTML.\n",
    "        label (str): The label to search for.\n",
    "\n",
    "    Returns:\n",
    "        str: The extracted value, or None if not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        label_element = soup.find(string=re.compile(rf\"\\s*{label}\\s*\", re.IGNORECASE)) #Finds the label using regex ignoring case\n",
    "        if label_element:\n",
    "            value_element = label_element.find_next(string=True) #Gets the next string\n",
    "            if value_element:\n",
    "                return value_element.strip()\n",
    "\n",
    "        label_element = soup.find('strong', string=re.compile(rf\"\\s*{label}\\s*\", re.IGNORECASE)) #Finds the label inside strong tags using regex ignoring case\n",
    "        if label_element:\n",
    "            value_element = label_element.find_next(string=True) #Gets the next string\n",
    "            if value_element:\n",
    "                return value_element.strip()\n",
    "\n",
    "        value_element = soup.find('span', class_=re.compile(rf\".*{label}.*\", re.IGNORECASE)) #Finds the value in a span tag with a class that contains the label using regex ignoring case\n",
    "        if value_element:\n",
    "            return value_element.text.strip()\n",
    "        \n",
    "        return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Main execution block\n",
    "if __name__ == \"__main__\":\n",
    "    base_url = \"https://www.lalonjapropiedadraiz.com/inmuebles/Venta/clases_Casa\"\n",
    "    total_pages = get_total_pages(base_url)\n",
    "\n",
    "    if total_pages:\n",
    "        all_page_links = get_all_page_links(base_url, total_pages)\n",
    "        all_listings = []\n",
    "        for page_link in all_page_links:\n",
    "            print(f\"Scraping listings from: {page_link}\")\n",
    "            listings_from_page = get_listing_links(page_link, base_url)\n",
    "            all_listings.extend(listings_from_page)\n",
    "            time.sleep(1)\n",
    "\n",
    "        all_listings_data = []\n",
    "\n",
    "        for listing_info in all_listings:\n",
    "            print(f\"Scraping data from: {listing_info['link']}\")\n",
    "            listing_data = get_listing_data(listing_info)\n",
    "            if listing_data:\n",
    "                all_listings_data.append(listing_data)\n",
    "            time.sleep(1)\n",
    "\n",
    "        if all_listings_data:\n",
    "            df = pd.DataFrame(all_listings_data) # Creates the pandas dataframe\n",
    "            # print(\"\\nExtracted data in Pandas DataFrame:\")\n",
    "            # print(df) # Prints the dataframe\n",
    "\n",
    "            # Create filename with date\n",
    "            extraction_date_str = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "            filename = f\"listings_data_{extraction_date_str}.csv\"  # Filename with date\n",
    "            df.to_csv(filename, index=False, encoding=\"utf-8\")  # Saves the dataframe to a csv file\n",
    "            print(f\"Data saved to {filename}\") #Prints the filename\n",
    "        else:\n",
    "            print(\"No listings with all the required data and a valid price were found.\")\n",
    "\n",
    "    elif total_pages == 0:\n",
    "        print(\"There are no pages\")\n",
    "    else:\n",
    "        print(\"Could not determine the total number of pages.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e00b59b-e141-4d1b-8fc8-8ba41a2d233b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
